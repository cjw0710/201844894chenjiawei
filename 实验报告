实验内容
测试sklearn中以下聚类算法在tweets数据集上的聚类效果。k-means, AffinityPropagation, mean-shift, SpectralClustering, AgglomerativeClustering, DBSCAN, GaussianMixtures。

实验环境
ubuntu16.04
python3.7
sklearn 0.19.2
实验原理
K-Means

首先随机选择k个聚类质心点，重复以下过程直到收敛：1、对于每个样例，选择距离其最近的聚类质心点作为其类别c。 2、对于每个类别c，计算所有包含样例的平均值作为新的质心。

Affinity Propagation

s(i,k):样本i和样本k之间的相似度。

r(i,k):吸引度。样本k适合作为样本i的聚类中心的累积信任度。

a(i,k):归属度。样本 i 应该选择样本 k 作为其聚类中心的累积信任度。

damping factor:阻尼系数，为了避免 r(i,k) 和 a(i,k) 在更新时发生数值震荡；

preference:偏好参数，相似度矩阵中横轴纵轴索引相同的点。

算法流程：（1）计算初始的相似度矩阵，将各点之间的吸引度 r(i,k) 和归属度 a(i,k) 初始化为 0；

（2）更新各点之间的吸引度，随之更新各点之间的归属度，公式如下：1按照lamda(阻尼系数)的比例进行更新。

（3）确定当前样本 i 的代表样本(exemplar)点 k，k就是使{a(i,k)+r(i,k)}取得最大值的那个 k； 重复步骤 2 和步骤 3，直到所有的样本的所属都不再变化为止；

Mean-Shift

对于集合中的每一个元素x，把该元素移动到它邻域中所有元素特征值的均值的位置，不断重复直到收敛，把该元素与它的收敛位置的元素标记为同一类。考虑到邻域中每个元素对x的重要程度不一样，距离近的元素对其的影响较大，采用高斯核函数，对各个元素进行加权并取平均值，求得偏移值。

Spectral Clustering

谱聚类就是先用Laplacian eigenmaps对数据进行降维（简单来说就是先将数据转换成邻接矩阵或相似性矩阵，再转换成Laplacian矩阵，再对Laplacian矩阵进行特征分解，把最小的K个特征向量排列在一起），然后再用k-means聚类。由于降纬的作用，它的效果通常比k-means好，计算复杂度低。

Agglomerative Clustering

层次聚类的方法。递归的合并能最小程度增加给定链接距离的一对簇。

两个簇的邻近度：

ward：定义为两个簇合并时导致的平方误差的增量。
average：定义为两个簇里所有元素的平均值之间的距离。
complete or maximum：将两个簇里距离最远的距离定义为两个簇的距离。
DBSCAN

首先计算每个点邻域中的点的个数，若该点邻域点的个数大于形成高密度区域所需的最小点数，则讲该点定义为核心点。如果p是核心点，则它与所有由它可达的点（包括核心点和非核心点）形成一个聚类，每个聚类有最少一个核心点，非核心点也可以是聚类的一部分，但它在聚类的[边缘]位置，因为它不能达至更多的点。img

Gaussian Mixtures

EM算法。观察数据x=(x_1, x_2, ... , x_m) 隐变量z。

首先初始化模型参数theta的初值theta_0。

E-step：Q_i(z^{(i)}) := P( z^{(i)}|x^{(i)}ï¼�\theta)) \

M-step：\theta : = arg \max \limits_{\theta}\sum\limits_{i=1}^m\sum\limits_{z^{(i)}}Q_i(z^{(i)})log{P(x^{(i)}ï¼� z^{(i)}|\theta)} \

重复EM步骤直到theta收敛。

Birch

首先需要建立CF树。有两个重要的参数：阈值T(判断是否吸收该节点入簇) 分支阈值L(最大分支个数)

1、从根节点向下寻找和新样本距离最近的叶子节点和叶子节点里最近的CF节点

2、如果新样本加入后，这个CF节点对应的超球体半径仍然满足小于阈值T，则更新路径上所有的CF三元组，插入结束。否则转入3.

3、如果当前叶子节点的CF节点个数小于阈值L，则创建一个新的CF节点，放入新样本，将新的CF节点放入这个叶子节点，更新路径上所有的CF三元组，插入结束。否则转入4。

4、将当前叶子节点划分为两个新叶子节点，选择旧叶子节点中所有CF元组里超球体距离最远的两个CF元组，分布作为两个新叶子节点的第一个CF节点。将其他元组和新样本元组按照距离远近原则放入对应的叶子节点。依次向上检查父节点是否也要分裂，如果需要按和叶子节点分裂方式相同。

所有的叶子节点里的样本点就是一个聚类的簇。

实验步骤
sklearn.cluster.KMeans => n_clusters 聚类个数 random_state 随机初始化
sklearn.cluster.AffinityPropagation ==> damping 阻尼系数，防止抖动
sklearn.cluster.MeanShift ==> bandwidth 高斯核的参数(类似于决定高纬球的半径) bin_seeding 优化加速
sklearn.cluster.SpectralClustering ==> n_clusters 聚类个数 random_state 随机初始化
sklearn.cluster.AgglomerativeClustering ==> n_clusters 聚类个数 linkage 确定计算距离的方法(ward, average, complete)
sklearn.cluster.DBSCAN ==> eps 确定邻域的大小 min_samples 密集区域所需最小样本点
sklearn.mixture.GaussianMixture ==> n_components 聚类个数
sklearn.cluster.Birch ==> n_clusters 聚类个数 threshold 高维球半径 branching_factor 分支个数
通过sklearn.metrics.cluster的normalized_mutual_info_score方法，计算得分，判断聚类好坏。

实验结果
K-Means score:0.779488

Affinity Propogation score:0.794534

Mean-Shift score:0.727855

Spectral Clustering score:0.780187

Agglomerative Clustering-ward score:0.783041

Agglomerative Clustering-complete score:0.755440

Agglomerative Clustering-average score:0.896244

DBSCAN score:0.656964

Gaussian Mixtures score:0.790252

Birch score:0.800036
